{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "First, let's make sure we have the needed packages installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numba in /Users/cmyers/mambaforge/envs/tutorial/lib/python3.10/site-packages (0.58.1)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /Users/cmyers/mambaforge/envs/tutorial/lib/python3.10/site-packages (from numba) (0.41.1)\n",
      "Requirement already satisfied: numpy<1.27,>=1.22 in /Users/cmyers/mambaforge/envs/tutorial/lib/python3.10/site-packages (from numba) (1.26.3)\n",
      "Requirement already satisfied: multiprocess in /Users/cmyers/mambaforge/envs/tutorial/lib/python3.10/site-packages (0.70.15)\n",
      "Requirement already satisfied: dill>=0.3.7 in /Users/cmyers/mambaforge/envs/tutorial/lib/python3.10/site-packages (from multiprocess) (0.3.7)\n",
      "Requirement already satisfied: numpy in /Users/cmyers/mambaforge/envs/tutorial/lib/python3.10/site-packages (1.26.3)\n"
     ]
    }
   ],
   "source": [
    "! pip install numba\n",
    "! pip install multiprocess\n",
    "! pip install numpy\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import numba\n",
    "import multiprocess\n",
    "from math import sqrt\n",
    "AU_2_EV = 27.211396 # convert from atomic units to electron volts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coulomb Interaction on Excited States\n",
    "Consider two molecules, each with excitation energies $E_1$ and $E_2$ and transition densities $\\rho_{eg}(\\mathbf{r}_1)$ and $\\rho_{eg}(\\mathbf{r}_2)$.  \n",
    "<img src=\"images/monomers.png\" width=\"400\" />  \n",
    "When combined, the supramolecular system have will have new excited states with a splitting $\\Delta$  \n",
    "<img src=\"images/splitting.png\" alt=\"Drawing\" style=\"width: 500px;\"/>  \n",
    "This splitting depends on their Coulomb coupling,  \n",
    "<img src=\"images/eq_coulomb.png\" alt=\"Drawing\" style=\"width: 300px;\"/>  \n",
    "This can be approximated by dividing the density into a grid and summing over each density \"cubes\"  \n",
    "<img src=\"images/eq_coulomb_approx.png\" alt=\"Drawing\" style=\"width: 150px;\"/>\n",
    "<img src=\"images/cube_sum.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "The following code examples will demonstrate various implementations of this summation  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Cube Data\n",
    "First we load in two transition density cube files for a cresyl-violet dimer. \n",
    "- Their geometries were oriented in a stacked configuration and optimized at the CAM-B3LYP/6-31G* level of theory with Gaussian 16.  \n",
    "- MultiWfn was used to generate the cube densities from a Gaussian formatted checkpoint file.  \n",
    "\n",
    "Because some of these examples will take some time to run, we will use two versions of the data; one with the \"regular\" number of cube points and another with a reduced number of points.\n",
    "\n",
    "Since the for loops scale by $\\mathcal{O}(NM)$ with $N$ and $M$ being the number of points in the first and second cube densities, we can approximate the time it takes to run the regular sized dataset by multiplying by the ratio of the two dataset sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading cube file\n",
      "Formatting\n",
      "Done\n",
      "Reading cube file\n",
      "Formatting\n",
      "Done\n",
      "Reading cube file\n",
      "Formatting\n",
      "Done\n",
      "Reading cube file\n",
      "Formatting\n",
      "Done\n",
      "Number of points in regular cubes:  (61617, 3) (66654, 3)\n",
      "                                    (61617,) (66654,)\n",
      "Number of points in reduced cubes:  (2975, 3) (5220, 3)\n",
      "                                    (2975,) (5220,)\n",
      "Point ratio:  264.46566328600403\n"
     ]
    }
   ],
   "source": [
    "import Cube # custom gaussian cube file reader\n",
    "from os.path import *\n",
    "\n",
    "data_1 = Cube.CubeData(join('CV_data', 'transdens_1_low.cub'))\n",
    "data_2 = Cube.CubeData(join('CV_data', 'transdens_2_low.cub'))\n",
    "dV_12 = data_1.dV * data_2.dV\n",
    "data_1_L = Cube.CubeData(join('CV_data', 'transdens_1_extra_low.cub'))\n",
    "data_2_L = Cube.CubeData(join('CV_data', 'transdens_2_extra_low.cub'))\n",
    "dV_12_L = data_1_L.dV * data_2_L.dV\n",
    "\n",
    "print(\"Number of points in regular cubes: \", data_1.coords.shape, data_2.coords.shape)\n",
    "print(\"                                   \", data_1.cube_data.shape, data_2.cube_data.shape)\n",
    "\n",
    "print(\"Number of points in reduced cubes: \", data_1_L.coords.shape, data_2_L.coords.shape)\n",
    "print(\"                                   \", data_1_L.cube_data.shape, data_2_L.cube_data.shape)\n",
    "\n",
    "point_ratio = data_1.n_points * data_2.n_points/(data_1_L.n_points * data_2_L.n_points)\n",
    "print(\"Point ratio: \", point_ratio)\n",
    "\n",
    "#   will store all of our benchmarks\n",
    "all_timers = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A First Attempt: Pure Python\n",
    "\n",
    "The doouble summation is implimented below in what would be considered the most straight forward way posssible:\n",
    "- Construct two for loops, one inner and one outer, for each cube densities.\n",
    "- Sum over the product of each cube divided by the distance between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Coulomb Integral 0.0 %\n",
      "    Coulomb Integral 20.0 %\n",
      "    Coulomb Integral 40.0 %\n",
      "    Coulomb Integral 60.0 %\n",
      "    Coulomb Integral 80.0 %\n",
      "pure_python: 5058.01 s (0.23374090007118797 eV)\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "import time\n",
    "\n",
    "def calc_coulomb_pure_python(pts_1, rho_1, pts_2, rho_2, dV):\n",
    "    from math import sqrt   #   normally, this is imported beforehand, but we need to fix fomr Jupyter multiprocessing bugs (discussed later)\n",
    "    total = 0.0\n",
    "    n_pts_1 = len(pts_1)\n",
    "    n_pts_2 = len(pts_2)\n",
    "    print_num = n_pts_1//5\n",
    "    for i in range(n_pts_1):\n",
    "        for j in range(n_pts_2):\n",
    "\n",
    "            x1, y1, z1 = pts_1[i]\n",
    "            x2, y2, z2 = pts_2[j]\n",
    "            dx = x1 - x2\n",
    "            dy = y1 - y2\n",
    "            dz = z1 - z2\n",
    "            r = sqrt(dx**2 + dy**2 + dz**2)\n",
    "            total += rho_1[i]*rho_2[j]/r\n",
    "\n",
    "        if i % print_num == 0:\n",
    "                print(f\"    Coulomb Integral {(i / n_pts_1*100):.1f} %\")\n",
    "\n",
    "    return total*dV\n",
    "\n",
    "#   Record the current time, run the code, and record the current time again.\n",
    "#   The difference in the two times measures the execution time.\n",
    "start = time.time()\n",
    "total = calc_coulomb_pure_python(data_1_L.coords, data_1_L.cube_data, data_2_L.coords, data_2_L.cube_data, dV_12_L)\n",
    "total_time = (time.time() - start)*point_ratio\n",
    "all_timers['pure_python'] = total_time\n",
    "print(f'pure_python: {total_time:.2f} s ({total*AU_2_EV} eV)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy\n",
    "\n",
    "Numpy gives us a way to manipulate multiple data points, stored in arrays, at the same time.\n",
    "- Think of numpy arrays as mathematical vectors and matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a + 2 = array([3, 4, 5, 6, 7])\n",
      " np.sum(a) = 15\n",
      " a * b = array([ 2,  6, 12, 20, 30])\n",
      " c @ d = array([[ 2,  6],\n",
      "       [ 6, 12]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([1, 2, 3, 4, 5])\n",
    "b = np.array([2, 3, 4, 5, 6])\n",
    "c = np.array([[1, 2], [3, 4]])\n",
    "d = np.array([[2, 0], [0, 3]])\n",
    "\n",
    "print(f\"{ a + 2 = }\")\n",
    "print(f\"{ np.sum(a) = }\")\n",
    "print(f\"{ a * b = }\")\n",
    "print(f\"{ c @ d = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run a line of python, you are actually running a bunch of compiled C++ code in the background  \n",
    "<img src=\"images/python_cpp.png\" width=\"600\" />  \n",
    "source: https://pythonextensionpatterns.readthedocs.io/en/latest/refcount.html  \n",
    "Each line must go through the Python interpreter to execute the correct `PyObject` and it's respective code.  \n",
    "\n",
    "Numpy functions are also implemented as compiled C-code, but each iteration within a function (np.sum, np.exp, etc.) is all done internally. This means the interpreter is only needed once for each numpy call!\n",
    "\n",
    "The next implementation replaces the inner loop with pure Numpy functions. Note the huge speed up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Coulomb Integral 0.0 %\n",
      "    Coulomb Integral 20.0 %\n",
      "    Coulomb Integral 40.0 %\n",
      "    Coulomb Integral 60.0 %\n",
      "    Coulomb Integral 80.0 %\n",
      "numpy: 55.75 s (0.2337409000711968 a.u.)\n"
     ]
    }
   ],
   "source": [
    "def calc_coulomb_numpy(pts_1, rho_1, pts_2, rho_2, dV):\n",
    "    total = 0.0\n",
    "    n_pts_1 = len(pts_1)\n",
    "    print_num = n_pts_1//5\n",
    "    for i in range(n_pts_1):\n",
    "        if i % print_num == 0:\n",
    "            print(f\"    Coulomb Integral {(i / n_pts_1*100):.1f} %\")\n",
    "\n",
    "        dr = pts_1[i] - pts_2\n",
    "        r = np.linalg.norm(dr, axis=1)\n",
    "        total += rho_1[i]*np.sum(rho_2/r)\n",
    "\n",
    "    return total*dV\n",
    "\n",
    "start = time.time()\n",
    "total = calc_coulomb_numpy(data_1_L.coords, data_1_L.cube_data, data_2_L.coords, data_2_L.cube_data, dV_12_L)\n",
    "total_time = (time.time() - start)*point_ratio\n",
    "\n",
    "#   depending on the estimated time with the reduced dataset, you may be able to uncomment \n",
    "#   out the next two lines and use the entire dataset if you are willing to wait.\n",
    "# total = calc_coulomb_numpy(data_1.coords, data_1.cube_data, data_2.coords, data_2.cube_data, dV_12)\n",
    "# total_time = (time.time() - start)\n",
    "\n",
    "all_timers['numpy'] = total_time\n",
    "print(f'numpy: {total_time:.2f} s ({total*AU_2_EV} a.u.)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multithreading\n",
    "\n",
    "Modern CPUs contain multiple CPU cores, each acting as an individual CPU, but all connected to the same internal memory (either CPU cache or RAM). Each core can perform (roughly) the same amount of work as another core. Unless your code is told to do so, python will run on only one core at a time.\n",
    "\n",
    "<img src=\"images/cpu.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "We tell python how to use these cores by setting up software \"threads\". Each thread is branch off of the main python program that does a separate amount of work and is assigned to one per CPU core each.\n",
    "- Technically, the number of threads are not limited by the number of cores, but with HPC, it's not beneficial to assign more. With some software, this can actually hurt your performance!  \n",
    "\n",
    "<img src=\"images/threading.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "We can split up our integral work by partitioning the outer loop (over molecule 1) into separate contributions of the total density. \n",
    "- Each sub-density will be assigned to a separate thread and will only affect the number of iterations in the outer for-loop.  \n",
    "- The inner for-loop will remain the same.  \n",
    "\n",
    "<img src=\"images/density_split.png\" alt=\"Drawing\" style=\"width: 400px;\"/>   \n",
    "And so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread 0 using indicies  0 4 8 12 ...\n",
      "Thread 1 using indicies  1 5 9 13 ...\n",
      "Thread 2 using indicies  2 6 10 14 ...\n",
      "    Coulomb Integral 0 0.0 %\n",
      "    Coulomb Integral 1 0.0 %\n",
      "    Coulomb Integral 2 0.0 %\n",
      "Thread 3 using indicies  3 7 11 15 ...\n",
      "    Coulomb Integral 3 0.0 %\n",
      "    Coulomb Integral 1 19.9 %\n",
      "    Coulomb Integral 2 19.9 %\n",
      "    Coulomb Integral 0 19.9 %\n",
      "    Coulomb Integral 3 19.9 %\n",
      "    Coulomb Integral 1 39.8 %\n",
      "    Coulomb Integral 0 39.8 %\n",
      "    Coulomb Integral 2 39.8 %\n",
      "    Coulomb Integral 3 39.8 %\n",
      "    Coulomb Integral 1 59.7 %\n",
      "    Coulomb Integral 0 59.7 %\n",
      "    Coulomb Integral 2 59.7 %    Coulomb Integral 3 59.8 %\n",
      "\n",
      "    Coulomb Integral 1 79.6 %\n",
      "    Coulomb Integral 2 79.6 %\n",
      "    Coulomb Integral 3 79.7 %\n",
      "    Coulomb Integral 0 79.6 %\n",
      "    Coulomb Integral 3 99.6 %\n",
      "    Coulomb Integral 0 99.5 %\n",
      "    Coulomb Integral 1 99.5 %\n",
      "    Coulomb Integral 2 99.5 %\n",
      "pure_python_threaded: 4978.33 s (0.23374090007119025 a.u.)\n",
      "pure_python:          5058.01 s \n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "\n",
    "#   Shared memory that each thread has access to\n",
    "thread_totals = np.array([])\n",
    "\n",
    "def _coulomb_by_indix(indicies, pts_1, rho_1, pts_2, rho_2, dV, thread_ID):\n",
    "\n",
    "    total = 0.0\n",
    "    n_pts_1 = len(indicies)\n",
    "    n_pts_2 = len(pts_2)\n",
    "    print_num = n_pts_1//5\n",
    "    for count, i in enumerate(indicies): # Now we loop over specified indicies only\n",
    "        for j in range(n_pts_2):\n",
    "            x1, y1, z1 = pts_1[i]\n",
    "            x2, y2, z2 = pts_2[j]\n",
    "            dx = x1 - x2\n",
    "            dy = y1 - y2\n",
    "            dz = z1 - z2\n",
    "            r = sqrt(dx**2 + dy**2 + dz**2)\n",
    "            total += rho_1[i]*rho_2[j]/r\n",
    "\n",
    "        if count % print_num == 0:\n",
    "            #   Also print thread ID with the progress\n",
    "            print(f\"    Coulomb Integral {thread_ID} {(count / n_pts_1*100):.1f} %\")\n",
    "\n",
    "    #   update the global integral totals\n",
    "    thread_totals[thread_ID] = total*dV\n",
    "\n",
    "def calc_coulomb_thread(n_threads, pts_1, rho_1, pts_2, rho_2, dV):\n",
    "    global thread_totals\n",
    "    all_threads = []\n",
    "    #   initialize each thread's total to zero\n",
    "    thread_totals = np.zeros(n_threads)\n",
    "    for n in range(n_threads):\n",
    "        #   these will be the indicies used by the inner Coulomb loop\n",
    "        indicies = np.arange(n, len(pts_1), n_threads)\n",
    "        print(f\"Thread {n} using indicies \", *indicies[0:4], \"...\")\n",
    "        \n",
    "        thread = threading.Thread(\n",
    "            target=_coulomb_by_indix, \n",
    "            args=(indicies, pts_1, rho_1, pts_2, rho_2, dV, n)\n",
    "            )\n",
    "        all_threads.append(thread)\n",
    "\n",
    "        #   the function is not called until we start the thread\n",
    "        thread.start()\n",
    "\n",
    "    #   now wait for all the threads to complete\n",
    "    for thread in all_threads:\n",
    "        thread.join()\n",
    "\n",
    "    #   The total Coulomb integral is equal to the sum of each thread's partial integral\n",
    "    return np.sum(thread_totals)\n",
    "\n",
    "start = time.time()\n",
    "total = calc_coulomb_thread(4, data_1_L.coords, data_1_L.cube_data, data_2_L.coords, data_2_L.cube_data, dV_12_L)\n",
    "# total = calc_coulomb_thread(4, data_1.coords, data_1.cube_data, data_2.coords, data_2.cube_data, dV_12)\n",
    "total_time = (time.time() - start)*point_ratio\n",
    "all_timers['pure_python_threaded'] = total_time\n",
    "print(f'pure_python_threaded: {total_time:.2f} s ({total*AU_2_EV} a.u.)')\n",
    "print(f'pure_python:          {all_timers[\"pure_python\"]:.2f} s ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Interpreter Lock\n",
    "\n",
    "You should have noticed that the above code didn't run any faster than the original pure-python function defined above. This is because every line in the function must obtain the Global Interpreter Lock (GIL) before it can be executed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "#   obtain GIL\n",
    "a = 5\n",
    "#   release GIL\n",
    "#   obtain GIL\n",
    "print(a)\n",
    "#   release GIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, we can replace the inner for-loop using Numpy routines, which release the GIL once executed. This allows another python line to be run while Numpy is handling the numerical heavy lifting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread 0 using indicies  0 4 8 12 ...\n",
      "    Coulomb Integral 0 0.0 %\n",
      "Thread 1 using indicies  1 5 9 13 ...\n",
      "    Coulomb Integral 1 0.0 %\n",
      "Thread 2 using indicies  2 6 10 14 ...\n",
      "    Coulomb Integral 2 0.0 %\n",
      "Thread 3 using indicies  3 7 11 15 ...\n",
      "    Coulomb Integral 3 0.0 %\n",
      "    Coulomb Integral 1 20.0 %\n",
      "    Coulomb Integral 0 20.0 %\n",
      "    Coulomb Integral 3 20.0 %\n",
      "    Coulomb Integral 2 20.0 %\n",
      "    Coulomb Integral 1 40.0 %\n",
      "    Coulomb Integral 0 40.0 %\n",
      "    Coulomb Integral 2 40.0 %\n",
      "    Coulomb Integral 3 40.0 %\n",
      "    Coulomb Integral 2 60.0 %\n",
      "    Coulomb Integral 0 60.0 %\n",
      "    Coulomb Integral 1 60.0 %\n",
      "    Coulomb Integral 3 60.0 %\n",
      "    Coulomb Integral 0 80.0 %\n",
      "    Coulomb Integral 3 80.0 %\n",
      "    Coulomb Integral 1 80.0 %\n",
      "    Coulomb Integral 2 80.0 %\n",
      "    Coulomb Integral 1 100.0 %\n",
      "    Coulomb Integral 3 100.0 %\n",
      "    Coulomb Integral 2 100.0 %\n",
      "numpy_threaded: 23.21 s (0.2422872335002314 a.u.)\n",
      "numpy:          55.75 s \n",
      "speed-up:       2.4x\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "\n",
    "thread_totals = np.array([])\n",
    "\n",
    "def _coulomb_by_indix_2(indicies, pts_1, rho_1, pts_2, rho_2, dV, thread_ID):\n",
    "    total = 0.0\n",
    "    n_pts_1 = len(indicies)\n",
    "    print_num = n_pts_1//5\n",
    "    for count, i in enumerate(indicies): # EDIT: loop over specified indicies only\n",
    "        if count % print_num == 0:\n",
    "            print(f\"    Coulomb Integral {thread_ID} {(count / n_pts_1*100):.1f} %\")\n",
    "\n",
    "        dr = pts_2 - pts_1[i]\n",
    "        r = np.linalg.norm(dr, axis=1)\n",
    "        total += rho_1[i]*np.sum(rho_2/r)\n",
    "\n",
    "    thread_totals[thread_ID] = total*dV\n",
    "\n",
    "def calc_coulomb_thread(n_threads, pts_1, rho_1, pts_2, rho_2, dV):\n",
    "    global thread_totals\n",
    "    all_threads = []\n",
    "    thread_totals = np.zeros(n_threads)\n",
    "    for n in range(n_threads):\n",
    "        #   these will be the indicies used by the inner Coulomb loop\n",
    "        indicies = np.arange(n, len(pts_1), n_threads)\n",
    "        print(f\"Thread {n} using indicies \", *indicies[0:4], \"...\")\n",
    "        \n",
    "        thread = threading.Thread(\n",
    "            target=_coulomb_by_indix_2, \n",
    "            args=(indicies, pts_1, rho_1, pts_2, rho_2, dV, n)\n",
    "            )\n",
    "        all_threads.append(thread)\n",
    "\n",
    "        #   the function is not called until we start the thread\n",
    "        thread.start()\n",
    "\n",
    "    #   now wait for all the threads to complete\n",
    "    for thread in all_threads:\n",
    "        thread.join()\n",
    "\n",
    "    #   The total Coulomb integral is equal to the sum of each thread's partial integral\n",
    "    return np.sum(thread_totals)\n",
    "\n",
    "start = time.time()\n",
    "total = calc_coulomb_thread(4, data_1.coords, data_1.cube_data, data_2.coords, data_2.cube_data, dV_12)\n",
    "total_time = (time.time() - start)\n",
    "all_timers['numpy_threaded'] = total_time\n",
    "print(f'numpy_threaded: {total_time:.2f} s ({total*AU_2_EV} a.u.)')\n",
    "print(f'numpy:          {all_timers[\"numpy\"]:.2f} s ')\n",
    "print(f'speed-up:       {all_timers[\"numpy\"]/all_timers[\"numpy_threaded\"]:.1f}x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiprocessing\n",
    "Similar to threading, `multiprocessing` also partitions the work onto multiple computing kernels termed `processes`.  \n",
    "\n",
    "The main difference here is that each process is a separate python instance. As each process is (mostly) independent from each other, they will each need a copy of the data being summed over.  \n",
    "<img src=\"images/multiprocess.png\" style=\"width: 700px\"/>\n",
    "-  There are ways to share data between each process using Queues, but this will not be covered here.  \n",
    "\n",
    "You can observe each Python instance running by monitoring you system with `top` bash command. As it runs, you should see something like this:  \n",
    "<img src=\"images/top_mp.png\" style=\"width: 400px\"/>\n",
    "- Notice that each process uses about the name amount of RAM. Keep this in mind if your datasets are very large or use many, many processes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Coulomb Integral 0.0 %\n",
      "    Coulomb Integral 0.0 %\n",
      "    Coulomb Integral 0.0 %\n",
      "    Coulomb Integral 0.0 %\n",
      "    Coulomb Integral 20.0 %\n",
      "    Coulomb Integral 20.0 %    Coulomb Integral 20.0 %\n",
      "\n",
      "    Coulomb Integral 20.0 %\n",
      "    Coulomb Integral 40.0 %    Coulomb Integral 40.0 %\n",
      "\n",
      "    Coulomb Integral 40.0 %\n",
      "    Coulomb Integral 40.0 %\n",
      "    Coulomb Integral 60.0 %\n",
      "    Coulomb Integral 60.0 %\n",
      "    Coulomb Integral 60.0 %\n",
      "    Coulomb Integral 60.0 %\n",
      "    Coulomb Integral 80.0 %\n",
      "    Coulomb Integral 80.0 %\n",
      "    Coulomb Integral 80.0 %\n",
      "    Coulomb Integral 80.0 %\n",
      "    Coulomb Integral 100.0 %\n",
      "    Coulomb Integral 100.0 %\n",
      "    Coulomb Integral 100.0 %\n",
      "multiprocess_numpy: 13.82 s (0.24228723350023043 a.u.)\n",
      "numpy:             55.75 s \n",
      "speed-up:          4.0x\n"
     ]
    }
   ],
   "source": [
    "import multiprocess as mp # use with Jupyter Notebooks\n",
    "#import multiprocess as mp # use with traditional python files\n",
    "\n",
    "def calc_coulomb_MP(n_proc, pts_1, rho_1, pts_2, rho_2, dV):\n",
    "    #   outer loop will be split by each process\n",
    "    pts_1_split = np.array_split(pts_1, n_proc)\n",
    "    rho_1_split = np.array_split(rho_1, n_proc)\n",
    "\n",
    "    #   inner loop will remain the same, so we simply copy the data\n",
    "    pts_2_copies = [pts_2]*n_proc\n",
    "    rho_2_copies = [rho_2]*n_proc\n",
    "\n",
    "    #   a copy also needs to be supplied to each process\n",
    "    dV_list = [dV]*n_proc\n",
    "\n",
    "    with mp.Pool(n_proc) as pool:\n",
    "        func_params = zip(pts_1_split, rho_1_split, pts_2_copies, rho_2_copies, dV_list)\n",
    "        # results = pool.starmap(calc_coulomb_pure_python, func_params)\n",
    "        results = pool.starmap(calc_coulomb_numpy, func_params)\n",
    "\n",
    "    return np.sum(results)\n",
    "\n",
    "# calc_coulomb_pure_python(data_1_L.coords, data_1_L.cube_data, data_2_L.coords, data_2_L.cube_data, dV_12_L)\n",
    "\n",
    "start = time.time()\n",
    "# total = calc_coulomb_MP(4, data_1_L.coords, data_1_L.cube_data, data_2_L.coords, data_2_L.cube_data, dV_12_L)\n",
    "# total_time = (time.time() - start)*point_ratio\n",
    "total = calc_coulomb_MP(4, data_1.coords, data_1.cube_data, data_2.coords, data_2.cube_data, dV_12)\n",
    "total_time = (time.time() - start)\n",
    "all_timers['multiprocess_numpy'] = total_time\n",
    "print(f'multiprocess_numpy: {total_time:.2f} s ({total*AU_2_EV} a.u.)')\n",
    "print(f'numpy:             {all_timers[\"numpy\"]:.2f} s ')\n",
    "print(f'speed-up:          {all_timers[\"numpy\"]/all_timers[\"multiprocess_numpy\"]:.1f}x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numba\n",
    "\n",
    "For functions that are mostly numerical (like our Coulomb integral), numba can be used to perform `just-in-time` compilation.  \n",
    "- As soon as the function is called, it is compiled with into machine code and run as any other self-contained python function.\n",
    "- Because the compilation can take some time, benchmarks should be run after the function is called at least once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numba:      5.93 s (0.24228723349036543 a.u.)\n",
      "pure_python:  5058.01 s\n",
      "speed-up:       853.3x\n"
     ]
    }
   ],
   "source": [
    "import numba\n",
    "\n",
    "#   numba decorator signials to compile the code\n",
    "#   fastmath=True enables SIMD vectorization\n",
    "@numba.jit(nopython=True, fastmath=True)   \n",
    "def calc_coulomb_numba(pts_1, rho_1, pts_2, rho_2, dV):\n",
    "    total = 0.0\n",
    "    n_pts_1 = len(pts_1)\n",
    "    n_pts_2 = len(pts_2)\n",
    "    for i in numba.prange(n_pts_1):\n",
    "        x1, y1, z1 = pts_1[i]\n",
    "        for j in range(n_pts_2):\n",
    "            \n",
    "            x2, y2, z2 = pts_2[j]\n",
    "            dx = x1 - x2\n",
    "            dy = y1 - y2\n",
    "            dz = z1 - z2\n",
    "            r = sqrt(dx*dx + dy*dy + dz*dz)\n",
    "            total += rho_1[i]*rho_2[j]/r\n",
    "            \n",
    "    return total*dV\n",
    "\n",
    "start = time.time()\n",
    "total = calc_coulomb_numba(data_1.coords, data_1.cube_data, data_2.coords, data_2.cube_data, dV_12)\n",
    "total_time = (time.time() - start)\n",
    "all_timers['numba'] = total_time\n",
    "print(f'numba:  {total_time:8.2f} s ({total*AU_2_EV} a.u.)')\n",
    "print(f'pure_python: {all_timers[\"pure_python\"]:8.2f} s')\n",
    "print(f'speed-up:    {all_timers[\"pure_python\"]/all_timers[\"numba\"]:8.1f}x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numba Parallelization\n",
    "Numba can also be used for multithreaded workloads. Simply add the `parallel=True` argument to the jit decorator and replace `range` with `numba.prange`. This last part is crutial, as it tells Numba where to parallelize the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numba_parallel: 1.64 s (0.24228723350222542 a.u.)\n",
      "numba:          5.93 s (0.24228723350222542 a.u.)\n",
      "speed-up:       853.3x\n"
     ]
    }
   ],
   "source": [
    "#   notice the parallel=True argument\n",
    "@numba.jit(nopython=True, fastmath=True, parallel=True)\n",
    "def calc_coulomb_numba_parallel(pts_1, rho_1, pts_2, rho_2, dV):\n",
    "    total = 0.0\n",
    "    n_pts_1 = len(pts_1)\n",
    "    n_pts_2 = len(pts_2)\n",
    "    for i in numba.prange(n_pts_1): # prange tells numba where to parallelize the loop\n",
    "        for j in range(n_pts_2):\n",
    "\n",
    "            x1, y1, z1 = pts_1[i]\n",
    "            x2, y2, z2 = pts_2[j]\n",
    "            dx = x1 - x2\n",
    "            dy = y1 - y2\n",
    "            dz = z1 - z2\n",
    "            r = sqrt(dx**2 + dy**2 + dz**2)\n",
    "            total += rho_1[i]*rho_2[j]/r\n",
    "            \n",
    "    return total*dV\n",
    "\n",
    "start = time.time()\n",
    "numba.set_num_threads(4)\n",
    "total = calc_coulomb_numba_parallel(data_1.coords, data_1.cube_data, data_2.coords, data_2.cube_data, dV_12)\n",
    "total_time = (time.time() - start)\n",
    "all_timers['numba_parallel'] = total_time\n",
    "print(f'numba_parallel: {total_time:.2f} s ({total*AU_2_EV} a.u.)')\n",
    "print(f'numba:          {all_timers[\"numba\"]:.2f} s ({total*AU_2_EV} a.u.)')\n",
    "print(f'speed-up:    {all_timers[\"pure_python\"]/all_timers[\"numba\"]:8.1f}x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combination of Numba and Multiprocessing\n",
    "Multiple forms of optimization can also be combined! \n",
    "\n",
    "For example, we can call the parallel Numba compiled code above with multiple threads, but also spread each set of threaded work over multiple processes. I'm using 4 threads and two processes, but this can be tweaked based on your available hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiproc + numba_parallel: 1.12 s (0.008903888411341872 a.u.)\n"
     ]
    }
   ],
   "source": [
    "def calc_coulomb_MP_numba(n_proc, pts_1, rho_1, pts_2, rho_2, dV):\n",
    "    #   outer loop will be split by each process\n",
    "    pts_1_split = np.array_split(pts_1, n_proc)\n",
    "    rho_1_split = np.array_split(rho_1, n_proc)\n",
    "\n",
    "    #   inner loop will remain the same, so we simply copy the data\n",
    "    pts_2_copies = [pts_2]*n_proc\n",
    "    rho_2_copies = [rho_2]*n_proc\n",
    "\n",
    "    #   a copy also needs to be supplied to each process\n",
    "    dV_list = [dV]*n_proc\n",
    "\n",
    "    with mp.Pool(n_proc) as pool:\n",
    "        func_params = zip(pts_1_split, rho_1_split, pts_2_copies, rho_2_copies, dV_list)\n",
    "        results = pool.starmap(calc_coulomb_numba_parallel, func_params) #   call the numba version with multiple threads\n",
    "\n",
    "    return np.sum(results)\n",
    "\n",
    "start = time.time()\n",
    "numba.set_num_threads(4)    #   set the number of threads\n",
    "total = calc_coulomb_MP_numba(2, data_1.coords, data_1.cube_data, data_2.coords, data_2.cube_data, dV_12)\n",
    "total_time = (time.time() - start)\n",
    "all_timers['multiproc + numba_parallel'] = total_time\n",
    "print(f'multiproc + numba_parallel: {total_time:.2f} s ({total} a.u.)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method                          Time\n",
      "--------------------------------------\n",
      "pure_python                  5058.01 s\n",
      "numpy                          55.75 s\n",
      "pure_python_threaded         4978.33 s\n",
      "numpy_threaded                 23.21 s\n",
      "multiprocess_numpy             13.82 s\n",
      "numba                           5.93 s\n",
      "numba_parallel                  1.64 s\n",
      "multiproc + numba_parallel      1.12 s\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(f'{\"Method\":28s} {\"Time\":>7s}')\n",
    "print('--------------------------------------')\n",
    "for name, val in all_timers.items():\n",
    "    print(f'{name:28s} {val:7.2f} s')\n",
    "print('--------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
